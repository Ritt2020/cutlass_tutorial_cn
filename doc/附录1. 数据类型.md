## 整数类型
### int8 / uint8
+ **定义**：8 位有符号 / 无符号整数。
+ **用途**：常用于深度学习推理中的量化计算（特别是 CNN / Transformer），在保证精度的同时减少显存占用、提升吞吐率。
+ **支持情况**：CUDA Tensor Cores 对 int8 有原生支持，CUTLASS 可用其构建高效 GEMM。

### int4 / uint4
+ **定义**：4 位有符号 / 无符号整数。
+ **用途**：主要用于极致压缩的量化推理，常见于大模型推理优化。
+ **支持情况**：在新架构（如 Hopper、Blackwell）上通过特殊的打包方式存储和计算。

### bin1_t
+ **定义**：1 位二元整数（取值只有 0 或 1）。
+ **用途**：适用于二值神经网络（BNN），极大压缩参数与计算量。
+ **支持情况**：需专门的 bit-level 优化，CUTLASS 提供类型支持。

---

## 浮点类型
### float (FP32)
+ **定义**：IEEE 754 单精度浮点数。
+ **用途**：最常用的训练数据类型，兼顾数值精度与性能。
+ **支持情况**：所有 CUDA 架构都支持，Tensor Core 可以加速 FP32 -> FP32 累加。

### double (FP64)
+ **定义**：IEEE 754 双精度浮点数。
+ **用途**：科学计算、高精度数值模拟等。
+ **支持情况**：在 A100 等数据中心 GPU 上有较好支持，但吞吐率远低于 FP32。

### half_t (FP16)
+ **定义**：IEEE 半精度浮点数（16 位）。
+ **用途**：深度学习中常用的混合精度训练 / 推理，提高速度，降低显存开销。
+ **支持情况**：Volta 及之后的 GPU Tensor Core 原生支持。

### bfloat16_t (BF16)
+ **定义**：16 位浮点数，指数位与 FP32 相同，尾数位减少。
+ **用途**：在保持 FP32 动态范围的同时节省存储空间，常用于训练。
+ **支持情况**：Ampere 及更新的 Tensor Core 原生支持。

### tfloat32_t (TF32)
+ **定义**：TensorFloat-32，是 NVIDIA 定义的 19 位浮点格式。
+ **用途**：在 FP32 动态范围下，使用 10 位尾数，适合训练加速。
+ **支持情况**：Ampere Tensor Core 原生支持。

### float_e5m2, float_e4m3, float_e3m2, float_e2m3, float_e2m1
+ **定义**：4~8 位子字节浮点类型，不同格式表示指数位 / 尾数位的组合。
+ **用途**：极低精度存储和计算，主要用于大模型推理的极限压缩。
+ **支持情况**：Blackwell 等新架构 GPU 上可用，通过 UMMA 指令支持。

---

## 其他类型
### Mixed Precision 类型
+ **定义**：A、B 输入和累加器（accumulator）使用不同数据类型，例如 A/B 用 int8，accumulator 用 int32。
+ **用途**：兼顾低精度计算的性能和高精度累加的准确性。
+ **支持情况**：CUTLASS 提供模板接口，可以灵活组合不同数据类型。

### Type-Erased Dynamic 类型
+ **定义**：如 `type_erased_dynamic_float8_t`，在运行时决定具体的浮点子格式。
+ **用途**：便于实现统一的 GEMM 接口，支持在运行时选择数据精度，而不是编译期固定。
+ **支持情况**：常见于子字节浮点（FP4、FP6、FP8）的场景。

### Complex 类型
+ **定义**：复数类型（如 `complex<float>`）。
+ **用途**：信号处理、科学计算等需要复数运算的领域。
+ **支持情况**：CUTLASS 提供复数 GEMM 支持，但性能可能低于实数类型。

